{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasagne Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document contains a series of notes on using Lasagne, starting with the sample code given in the Lasagne Github page. For more information on Lasagne, visit the Github page here: https://github.com/Lasagne/Lasagne "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example is from the Lasagne Github. It is not functional at the moment (given that there is no data to fill the input), but it demonstrates a basic example of how to use Lasagne to structure a simple neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    This layer holds a symbolic variable that represents a network input. A\n",
      "    variable can be specified when the layer is instantiated, else it is\n",
      "    created.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    shape : tuple of `int` or `None` elements\n",
      "        The shape of the input. Any element can be `None` to indicate that the\n",
      "        size of that dimension is not fixed at compile time.\n",
      "\n",
      "    input_var : Theano symbolic variable or `None` (default: `None`)\n",
      "        A variable representing a network input. If it is not provided, a\n",
      "        variable will be created.\n",
      "\n",
      "    Raises\n",
      "    ------\n",
      "    ValueError\n",
      "        If the dimension of `input_var` is not equal to `len(shape)`\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The first dimension usually indicates the batch size. If you specify it,\n",
      "    Theano may apply more optimizations while compiling the training or\n",
      "    prediction function, but the compiled function will not accept data of a\n",
      "    different batch size at runtime. To compile for a variable batch size, set\n",
      "    the first shape element to `None` instead.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from lasagne.layers import InputLayer\n",
      "    >>> l_in = InputLayer((100, 20))\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import lasagne \n",
    "import theano \n",
    "import theano.tensor as T \n",
    "\n",
    "\n",
    "# Sample neural network from the Lasagne github. \n",
    "input_var = T.tensor4('X')\n",
    "target_var = T.ivector('y')\n",
    "\n",
    "# Create a convolutional neural network! \n",
    "from lasagne.nonlinearities import leaky_rectify, softmax \n",
    "network = lasagne.layers.InputLayer((None, 3, 32, 32), input_var)\n",
    "network = lasagne.layers.Conv2DLayer(\n",
    "    network, \n",
    "    64, \n",
    "    (3, 3), \n",
    "    nonlinearity=leaky_rectify\n",
    ")\n",
    "\n",
    "network = lasagne.layers.Conv2DLayer(\n",
    "    network, \n",
    "    32, \n",
    "    (3, 3), \n",
    "    nonlinearity=leaky_rectify\n",
    ")\n",
    "\n",
    "network = lasagne.layers.Pool2DLayer(\n",
    "    network,\n",
    "    (3, 3),\n",
    "    stride=2,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "network = lasagne.layers.DenseLayer(\n",
    "    lasagne.layers.dropout(network, 0.5),\n",
    "    128, \n",
    "    nonlinearity=leaky_rectify,\n",
    "    W=lasagne.init.Orthogonal()\n",
    ")\n",
    "\n",
    "network = lasagne.layers.DenseLayer(\n",
    "    lasagne.layers.dropout(network, 0.5),\n",
    "    10,\n",
    "    nonlinearity=softmax\n",
    ")\n",
    "\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, \n",
    "                                                   target_var)\n",
    "loss = loss.mean() + 1e-4 * lasagne.regularization.regularize_network_params(\n",
    "    network, lasagne.regularization.12)\n",
    "\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.neterov_momentum(loss, params, \n",
    "                                          learning_rate=0.01,\n",
    "                                          momentum=0.9)\n",
    "train_fn = theano.function([input_var, target_var], \n",
    "                          loss, updates=updates)\n",
    "for epoch in range(100):\n",
    "    loss = 0\n",
    "    for input_batch, target_batch in  training_data:\n",
    "        loss += train_fn(input_batch, target_batch)\n",
    "    print(\"Epock %d: Loss %g\" % (epoch + 1, loss / len(training_data)))\n",
    "    \n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "predict_fn = theano.function([input_var], T.argmax(test_prediction, axis=1))\n",
    "print(\"Predicted class for first test input: %r\" % predict_fn(test_data[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code may take some explanation, so let's do that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InputLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An input layer is the first layer in the network. It is the starting point for all data, and sets the initial dimensions. Every layer type in Lasagne has a large number of potential inputs to specify their dimensions and behaviour. An explanation of all of the potential parameters for any given layer type can be found the in the __doc__ for that layer. \n",
    "\n",
    "In particular, the Input Layer is supposed to be the first layer in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv2DLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Conv2DLayer class is a description of a 2-dimensional Convolutional Layer. Convolutional layers are sparsely-connected (read, not fully connected) layers where each output node only depends on some marginal fraction of the input, instead of every input node. Obviously, as entropy increases the time complexity of any neural network algorithm would blaze on for all eternity if it were fully-connected, so convolutional layers are meant to reduce the total number of dependencies and VASTLY reduce the run-time for algorithms such as this. Large-scale image recognition makes use of convolutional layers for this purpose.\n",
    "\n",
    "Convolutional layers are made up of some number of filters. Technically there is one filter for every output node, and that filter has some dimension, ie. the number of inputs it receives from the input layer, and a stride, which means how many nodes it skips. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pool2DLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer type is meant to describe pooling layers, which are meant to cut down the total number of inputs for the network. Pooling layers are meant to reduce the complexity of the input before the final prediction is made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DropoutLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer type describes a dropout layer, which is a layer that has some probability for a particular input becoming 0. It randomly drops out certain inputs. The reasoning behind this is above my current pay-grade at the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A DenseLayer is a layer that is fully-connected to the layer behind it. These can be seen as the direct analog of the Conv2DLayer's above, since whereas convolutional layers are only sparsely connected using filters, dense layers are densely connected, and every output is connected to every input. In the example above, the dense layers are only used toward the end since using them closer to the beginning, when there are far more nodes, would significantly increase the running time of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
