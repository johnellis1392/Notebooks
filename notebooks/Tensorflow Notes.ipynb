{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This page documents some simple notes to do with using TensorFlow. The full descriptions of some of the contained code examples can be found on the TensorFlow website,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic example of a tensorflow application is the following HelloWorld, which I will explain :3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, World!'\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "# Simply print Hello, World!\n",
    "hello = tf.constant('Hello, World!')\n",
    "session = tf.Session()\n",
    "print(session.run(hello))\n",
    "\n",
    "# Run a simple math computation\n",
    "a = tf.constant(10)\n",
    "b = tf.constant(10)\n",
    "print(session.run(a + b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This application simply creates some constants and passes those commands to the TensorFlow session to compute, returning the result from the Session.run() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Beginner Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example does some simple hand-written digit analysis on tensorflow's example dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "0.9105\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data \n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# mnist now consists of three training sets:\n",
    "# - train      # for training data \n",
    "# - test       # for testing \n",
    "# - validation # for validating results \n",
    "# \n",
    "# mnist.train.images contains all the images for the computation \n",
    "# mnist.train.labels contains labels for stuff? \n",
    "\n",
    "# A placeholder is a value we expect tensorflow to calculate \n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "# Next we instantiate some variables for sending through tf \n",
    "# Here, w is the weights and b is the biases \n",
    "w = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Now calculate our model using softmax regression. This is the \n",
    "# neural network formula, basically, which is just all the weights\n",
    "# of the network multiplied by the inputs, plus the biases. \n",
    "y = tf.nn.softmax(tf.matmul(x, w) + b)\n",
    "\n",
    "# Create another placeholder for storing the correct answers. \n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# Now we calculate the cross-entropy! Which is the calculation of \n",
    "# the correctness of the model. \n",
    "cross_entropy = -tf.reduce_sum(y_ * tf.log(y))\n",
    "\n",
    "# Now we calculate the actual backpropogation algorithm. \n",
    "# Here 0.01 is the training rate, and minimize is the \n",
    "# function meant to calculate the final value. \n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "# Now we initialize the variables for our model. \n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Create a new session. We interact with tensorflow through\n",
    "# session objects that store the abstract syntax tree created\n",
    "# up to this point. Session.run(...) is used to execute a \n",
    "# constructed command. \n",
    "session = tf.Session()\n",
    "session.run(init)\n",
    "\n",
    "# Now we run our training step 1000 times! \n",
    "for i in range(1000):\n",
    "    # Grab next 100 elements \n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    # Run the training step \n",
    "    session.run(train_step, feed_dict={x: batch_xs, y_: batch_ys}) \n",
    "    \n",
    "\n",
    "# Now we match the predicted values against the actual values \n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "\n",
    "# Now we determine what number of predictions were correct \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Print the accuracy of the test data \n",
    "print(session.run(\n",
    "        accuracy, \n",
    "        feed_dict={\n",
    "            x: mnist.test.images, \n",
    "            y_: mnist.test.labels\n",
    "        }))\n",
    "\n",
    "# ACCURACY OF 91% YO! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced MNIST Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move on to the more complicated material; using a deep convolutional neural network to classify digits from the above dataset.\n",
    "\n",
    "First we start by loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to import tensorflow and start a new interactive session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "session = tf.InteractiveSession() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example first starts by creating a simple single-layer network using a softmax regression, then extends this model into a full multi-layer convolutional network. First we need to start by Creating some placeholder values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x  = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, x denotes the input images from the MNIST dataset; each image flattens out into a 784-dimensional vector, so the dimension [None, 784] denotes a 1-dimensional vector. y_ denotes the output values for the given x vector image, which we will use to compare against our trained network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our weights and biases for the network, as with the above example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b81ebcb69199>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the resultant network is only going to be one-dimensional, we simply have one layer casting the dimensions of the input to teh dimensions of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to initialize the variables used in our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will initialize all of the Variable objects used in our computation before we begin. \n",
    "\n",
    "Finally, we Create our network by multiplying all of the weights together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then calculate the cross entropy of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = -tf.reduce_sum(y_ * tf.log(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now again we do the training, using a gradient descent optimizer with a weight of 0.01:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "for i in range(1000):\n",
    "    batch = mnist.next_batch(50)\n",
    "    train_step.run(feed_dict={\n",
    "        x: batch[0],\n",
    "        y_: batch[1]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can get some heuristics on how well the model did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cas(correct_prediction, tf.float32))\n",
    "print(accuracy.eval(feed_dict={\n",
    "            x: mnist.test.images,\n",
    "            y_: mnist.test.labels\n",
    "        }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build a multi-layer convolutional neural network! Yay :D\n",
    "\n",
    "First we start by initializing our weights and biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    intial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions will take some given shape and create the corresponding matrices for representing the given data. \n",
    "\n",
    "The next step is to create some functions that will represent our convolution and pooling layers. This will be using max-pooling over 2x2 input blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(\n",
    "        x, \n",
    "        W, \n",
    "        strides=[1, 1, 1, 1], \n",
    "        padding='SAME'\n",
    "    )\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(\n",
    "        x,\n",
    "        ksize=[1,2,2,1],\n",
    "        strides=[1,2,2,1],\n",
    "        padding='SAME'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will initialize our first layers using these parameterized functions. The convolutional layer will compute 32 \"features\" for 5x5 filters from the previous layer. This is represented by the array [5, 5, 1, 32]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to reshape the input image into a 4-dimensional tensor. Here the dimensions are represented as such: [-1, 28, 28, 1], where the second and third dimensions represent the sizes of the images, and the final dimension represents the color channel (1, in this instance, since there are no RGB values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we feed the image into the input convolutional layer of the network and feed it into the ReLU (Rectified Linear Unit) function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create some fully-connected layers to finish the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add a dropout layer for preventing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we add a softmax layer for calculating the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get some heuristics from the model and report some statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "session.run(tf.initialize_all_variables())\n",
    "\n",
    "for i in range(20000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i % 100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            x: batch[0],\n",
    "            y_: batch[1],\n",
    "            keep_prob: 1.0\n",
    "        })\n",
    "        \n",
    "        print(\"step %d, training accuracy %g\" % (i, train_accuracy))\n",
    "    \n",
    "    train_step.run(feed_dict={\n",
    "        x: batch[0], \n",
    "        y_ batch[1],\n",
    "        keep_prob: 0.5\n",
    "    })\n",
    "    \n",
    "print(\"Test accuracy %g\" % accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images,\n",
    "    y_: mnist.test.labels, \n",
    "    keep_prob: 1.0 \n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to bring everything together into one final product. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Step 0, training accuracy 0.16\n",
      "Step 100, training accuracy 0.64\n",
      "Step 200, training accuracy 0.76\n",
      "Step 300, training accuracy 0.72\n",
      "Step 400, training accuracy 0.88\n",
      "Step 500, training accuracy 0.92\n",
      "Step 600, training accuracy 0.98\n",
      "Step 700, training accuracy 0.94\n",
      "Step 800, training accuracy 0.9\n",
      "Step 900, training accuracy 0.94\n",
      "Step 1000, training accuracy 0.92\n",
      "Step 1100, training accuracy 0.9\n",
      "Step 1200, training accuracy 0.98\n",
      "Step 1300, training accuracy 0.94\n",
      "Step 1400, training accuracy 0.92\n",
      "Step 1500, training accuracy 0.98\n",
      "Step 1600, training accuracy 0.9\n",
      "Step 1700, training accuracy 0.98\n",
      "Step 1800, training accuracy 0.96\n",
      "Step 1900, training accuracy 0.98\n",
      "Test accuracy 0.9787\n"
     ]
    }
   ],
   "source": [
    "# Retrieve MNIST Data \n",
    "from tensorflow.examples.tutorials.mnist import input_data \n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "# Create our tensorflow session\n",
    "import tensorflow as tf \n",
    "session = tf.InteractiveSession() \n",
    "\n",
    "x  = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "# W = tf.Variable(tf.zeros([784, 10]))\n",
    "# b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# session.run(tf.initialize_all_variables())\n",
    "\n",
    "# y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "# cross_entropy = -tf.reduce_sum(y_ * tf.log(y))\n",
    "# train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "# for i in range(1000):\n",
    "#     batch = mnist.train.next_batch(50)\n",
    "#     train_step.run(feed_dict={\n",
    "#         x: batch[0],\n",
    "#         y_: batch[1]\n",
    "#     })\n",
    "    \n",
    "# correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "# print(accuracy.eval(feed_dict={\n",
    "#     x: mnist.test.images,\n",
    "#     y_: mnist.test.labels\n",
    "# }))\n",
    "\n",
    "\n",
    "# Utility functions for creating network layers \n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial) \n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1],\n",
    "                         strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "\n",
    "# Some network layers \n",
    "W_conv1 = weight_variable([5,5,1,32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "\n",
    "\n",
    "W_conv2 = weight_variable([5,5,32,64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "\n",
    "\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1,7 * 7 * 64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "session.run(tf.initialize_all_variables())\n",
    "\n",
    "for i in range(2000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i % 100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            x: batch[0],\n",
    "            y_: batch[1],\n",
    "            keep_prob: 0.5\n",
    "        })\n",
    "        \n",
    "        print(\"Step %d, training accuracy %g\" % (i, train_accuracy))\n",
    "        \n",
    "    train_step.run(feed_dict={\n",
    "        x: batch[0],\n",
    "        y_: batch[1],\n",
    "        keep_prob: 0.5\n",
    "    })\n",
    "    \n",
    "print(\"Test accuracy %g\" % accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images,\n",
    "    y_: mnist.test.labels,\n",
    "    keep_prob: 1.0\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
